{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "tokenizerexperiments.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!brew install mecab mecab-ipadic git curl xz\n",
    "\n",
    "!sudo apt update -qy && apt install -qy mecab libmecab-dev mecab-ipadic-utf8\n",
    "!pip install -q konoha[all]\n",
    "!mkdir data && wget -q https://github.com/himkt/konoha/raw/master/data/model.spm -O data/model.spm\n",
    "! pip install neologdn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji945j5ngfYf"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "db4FhCLbZusn"
   },
   "source": [
    "import pandas as pd\n",
    "from konoha import SentenceTokenizer\n",
    "from konoha import WordTokenizer\n",
    "import re\n",
    "import neologdn\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "import string\n",
    "import numpy as np"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tDyZfj2VlkAR"
   },
   "source": [
    "SEED = 13\n",
    "np.random.seed(SEED)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1YIYL_C52nbI"
   },
   "source": [
    "filepath = \"/Users/kapeleshh/PycharmProjects/STS/\"\n",
    "df=  pd.read_csv(filepath + \"data/preproc_train.csv\")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0JtWTEab5dgs"
   },
   "source": [
    "df['tokmecab'] = df['text']\n",
    "df['tokjanome'] = df['text']\n",
    "df['toknagisa'] = df['text']\n",
    "#df['toksentencepiece'] = df['text']"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data for Japanese stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "file1 = open(filepath + \"stopwords/Japanese_stopword_list.txt\")\n",
    "\n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "stopJa = line.split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvOLJXDRgh1B"
   },
   "source": [
    "### Define Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sentence_tokenizer = SentenceTokenizer()\n",
    "tokenizers = [\"MeCab\", \"Janome\", \"nagisa\"]\n",
    "#tokenizers_support_postag = [\"MeCab\", \"Janome\", \"nagisa\"]\n",
    "\n",
    "word_tokenizers = []\n",
    "for word_tokenizer_name in tokenizers:\n",
    "    tokenizer = WordTokenizer(word_tokenizer_name)\n",
    "    word_tokenizers.append(tokenizer)\n",
    "\n",
    "#tokenizer = WordTokenizer(\"Sentencepiece\", model_path=\"./data/model.spm\")\n",
    "#word_tokenizers.append(tokenizer)\n",
    "\n",
    "# FIXME: `ContextualVersionConflict` happens on Google Colaboratory.\n",
    "#        If you want to try `Sudachi`, please restart this notebook\n",
    "#        after running the entire cells once.\n",
    "#\n",
    "# ref. https://github.com/nipy/niwidgets/issues/57\n",
    "#\n",
    "# tokenizer = WordTokenizer(\"Sudachi\", mode=\"A\", with_postag=True)\n",
    "# word_tokenizers.append(tokenizer)\n",
    "\n",
    "print(\"Finish creating word tokenizers\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdxqmrp_fa3E",
    "outputId": "c8d057eb-fa76-4d63-a079-60fb48709f11"
   },
   "source": [
    "#df = df[:100000]\n",
    "cols = ['tokmecab','tokjanome', 'toknagisa']\n",
    "for wt in range(len(word_tokenizers)):\n",
    "  for i in range(df.shape[0]):\n",
    "    #df.iloc[i] = df.iloc[i].to_string()\n",
    "    df[cols[wt]][i] = neologdn.normalize(df[cols[wt]][i])\n",
    "    df[cols[wt]][i] = word_tokenizers[wt].tokenize(df[cols[wt]][i])\n",
    "    df[cols[wt]][i] = [str(r) for r in df[cols[wt]][i]]\n",
    "    df[cols[wt]][i]= [w for w in df[cols[wt]][i] if(re.findall(r\"([a-zA-Z]+)\",w) or re.findall(r\"([ぁ-んァ-ン]+)\",w) or re.findall(r\"([一-龯]+)\",w) or re.findall(r\"(\\d+)\",w)) ]\n",
    "    #df[cols[wt]][i]= [w for w in df[cols[wt]][i] if(re.findall(r\"([a-zA-Z]+)\",w) or re.findall(r\"([ぁ-んァ-ン]+)\",w) or re.findall(r\"([一-龯]+)\",w) or re.findall(r\"(\\d+)\",w)) ]\n",
    "    df[cols[wt]][i] = [word for word in df[cols[wt]][i] if word not in sw_spacy and word not in string.punctuation and word not in stopJa]\n",
    " #  df[cols[wt]][i] = [word for word in df[cols[wt]][i] if word.lower() in eng_words or not word.isalpha()]\n",
    "    df[cols[wt]][i] = ' '.join(df[cols[wt]][i]).split()\n",
    "    print(cols[wt],i)\n",
    "  df[cols[wt]] = df[cols[wt]].apply(lambda y: np.nan if len(y)==0 else y)\n",
    "    #df = df[df[cols[wt]].notna()]\n",
    "    #result = [str(r) for r in result]\n",
    "    #print(\" \".join(result))"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write data into pickle and csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JPB6JOeQZjWa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dfc.to_csv(filepath + 'data/token_compare(full).csv', mode = 'w', index=False)\n",
    "df.to_pickle(filepath + 'data/pickle/token_compare(full)')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}