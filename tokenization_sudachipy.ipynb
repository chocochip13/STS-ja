{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Sudachtokenization.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vJPXQ34iyFZE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MW_m2g_CyFZJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068573576,
     "user_tz": -330,
     "elapsed": 2550,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    },
    "outputId": "8f2b8692-a110-4acf-e201-335f6411655b"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import neologdn\n",
    "\n",
    "import nltk\n",
    "#nltk.download('words')\n",
    "import spacy"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/kapeleshh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "omvYvO-gyFZK"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oCGnDrksyFZL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068592388,
     "user_tz": -330,
     "elapsed": 18815,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    }
   },
   "source": [
    "filepath = \"/Users/kapeleshh/PycharmProjects/STS/\"\n",
    "df=  pd.read_csv(filepath + \"data/preproc_train.csv\")\n",
    "#df = pd.read_csv(filepath + \"data/Ctoken_train.csv\")"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OiHnm9UNyFZL"
   },
   "source": [
    "### Load Data for Japanese stopwords"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "H23WMxImyFZM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068602937,
     "user_tz": -330,
     "elapsed": 427,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    }
   },
   "source": [
    "file1 = open(filepath + \"stopwords/Japanese_stopword_list.txt\")\n",
    "\n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "stopJa = line.split()"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load English Stop words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "\n",
    "en_alphabets = ['o','q','w','e','r','t','y','u','i','p','a','s','d','f','g','h','j','k','l','z','x','c','v','b','n','m']\n",
    "sw_spacy.update(en_alphabets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rRNXiYP1yFZN"
   },
   "source": [
    "### Sudachi Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oZpJ5MvfyFZN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068603610,
     "user_tz": -330,
     "elapsed": 3,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    }
   },
   "source": [
    "def surface_sudachitokens(df):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C #(Mode = B or C)\n",
    "    for i in range(df.shape[0]):\n",
    "        a = df.iloc[i].to_string()\n",
    "        a = neologdn.normalize(a)\n",
    "        df['text'][i] = [m.surface() for m in tokenizer_obj.tokenize(a, mode)]\n",
    "        print(i)\n",
    "    return df\n",
    "\n",
    "def normalized_sudachitokens(df):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C #(Mode = B or C)\n",
    "    for i in range(df.shape[0]):\n",
    "        a = df.iloc[i].to_string()\n",
    "        a = neologdn.normalize(a)\n",
    "        df['text'][i] = [m.normalized() for m in tokenizer_obj.tokenize(a, mode)]\n",
    "        print(i)\n",
    "    return df\n",
    "\n",
    "def stopwords_removal(df, en_stopwords, ja_stopwords, en_dictionary):\n",
    "    for i in range(df.shape[0]):\n",
    "        #df['text'][i]= [w for w in df['text'][i] if(re.findall(r\"([a-zA-Z]+)\",w) or re.findall(r\"([ぁ-んァ-ン]+)\",w) or re.findall(r\"([一-龯]+)\",w) or re.findall(r\"(\\d+)\",w)) or w.lower() in eng_words]\n",
    "        df['text'][i]= [w for w in df['text'][i] if(re.findall(r\"([ぁ-んァ-ン]+)\",w) or re.findall(r\"([一-龯]+)\",w) or re.findall(r\"(\\d+)\",w)) or w.lower() in en_dictionary or not w.isalpha()]\n",
    "        df['text'][i]= [w for w in df['text'][i] if(re.findall(r\"([a-zA-Z]+)\",w) or re.findall(r\"([ぁ-んァ-ン]+)\",w) or re.findall(r\"([一-龯]+)\",w) or re.findall(r\"(\\d+)\",w))]\n",
    "        df['text'][i] = [word for word in df['text'][i] if word not in en_stopwords and word not in string.punctuation and word not in ja_stopwords]\n",
    " #      df['text'][i] = [word for word in df['text'][i] if word.lower() in eng_words or not word.isalpha()]\n",
    "        df['text'][i] = ' '.join(df['text'][i]).split()\n",
    "        print(i)\n",
    "    df.text = df.text.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "    return df"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surface_sudachitokens(dfs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 text\n0                                                 NaN\n1                                         [りょう, むりょう]\n2                                                 NaN\n3                    [つぼみ, つい, ちいさく, くさっ, もう, かい, ませ]\n4                                                 NaN\n..                                                ...\n95  [そんな, よく, そんな, よく, そんな, よく, そんな, よく, そんな, よく, ...\n96  [いい, いい, いい, いい, いい, いい, いい, いい, いい, いい, いい, い...\n97  [うまうま, うまうま, うまうま, うまうま, うまうま, うまうま, うまうま, うまう...\n98                                              [グッド]\n99                                     [どい, けど, たのしみ]\n\n[100 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[りょう, むりょう]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[つぼみ, つい, ちいさく, くさっ, もう, かい, ませ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>[そんな, よく, そんな, よく, そんな, よく, そんな, よく, そんな, よく, ...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>[いい, いい, いい, いい, いい, いい, いい, いい, いい, いい, いい, い...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>[うまうま, うまうま, うまうま, うまうま, うまうま, うまうま, うまうま, うまう...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>[グッド]</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>[どい, けど, たのしみ]</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_removal(dfs, sw_spacy, stopJa, eng_words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}