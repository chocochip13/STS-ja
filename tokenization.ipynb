{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import nltk\n",
    "import neologdn\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words')\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "eng_words = set(nltk.corpus.words.words())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = \"/Users/kapeleshh/PycharmProjects/STS/\"\n",
    "df =  pd.read_csv(filepath + \"data/preproc_train.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data for Japanese stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file1 = open(\"stopwords/Japanese_stopword_list.txt\")\n",
    "\n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "stopJa = line.split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sudachi Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_obj = dictionary.Dictionary().create()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set mode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mode = tokenizer.Tokenizer.SplitMode.C #(Mode = B or C)\n",
    "# B - Med\n",
    "# C - NE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Surface tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#text = df['text']\n",
    "for i in range(dfs.shape[0]):\n",
    "    a = dfs.iloc[i].to_string()\n",
    "    a = neologdn.normalize(a)\n",
    "    dfs['text'][i] = [m.surface() for m in tokenizer_obj.tokenize(a, mode)]\n",
    "    dfs['text'][i]= [w for w in dfs['text'][i] if(re.findall(r\"([a-zA-Z]+)\",w) or re.findall(r\"([ぁ-んァ-ン]+)\",w) or re.findall(r\"([一-龯]+)\",w) or re.findall(r\"(\\d+)\",w)) ]\n",
    "    dfs['text'][i] = [word for word in dfs['text'][i] if word not in sw_spacy and word not in string.punctuation and word not in stopJa]\n",
    " #  dfs['text'][i] = [word for word in dfs['text'][i] if word.lower() in eng_words or not word.isalpha()]\n",
    "    dfs['text'][i] = ' '.join(dfs['text'][i]).split()\n",
    "    dfs.text = dfs.text.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "    dfs = dfs.dropna(dfs)\n",
    "#   dfs['text'][i]  = words\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalized Tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(dfn.shape[0]):\n",
    "    a = dfn.iloc[i].to_string()\n",
    "    dfn['text'][i] = [m.normalized_form() for m in tokenizer_obj.tokenize(a, mode)]\n",
    "    dfn['text'][i] = [word.lower() for word in dfn['text'][i]]\n",
    "    dfn['text'][i] = [word for word in dfn['text'][i] if word not in sw_spacy and word not in string.punctuation and word not in stopJa]\n",
    "    dfn['text'][i] = ' '.join(dfn['text'][i]).split()\n",
    "#   dfn['text'][i]  = words\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs.to_csv(filepath + 'data/Ctoken_surface_train.csv', mode = 'w', index=False)\n",
    "dfn.to_csv(filepath + 'data/Ctoken_normal_train.csv', mode = 'w', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}