{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Sudachtokenization.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vJPXQ34iyFZE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MW_m2g_CyFZJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068573576,
     "user_tz": -330,
     "elapsed": 2550,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    },
    "outputId": "8f2b8692-a110-4acf-e201-335f6411655b"
   },
   "source": [
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from konoha import WordTokenizer\n",
    "\n",
    "import nltk\n",
    "#nltk.download('words')\n",
    "import spacy"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "omvYvO-gyFZK"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%33\n"
    },
    "id": "oCGnDrksyFZL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068592388,
     "user_tz": -330,
     "elapsed": 18815,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    }
   },
   "source": [
    "filepath = \"/Users/kapeleshh/PycharmProjects/STS/\"\n",
    "df=  pd.read_csv(filepath + \"data/preproc_train.csv\")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OiHnm9UNyFZL"
   },
   "source": [
    "### Load Data for Japanese stopwords"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "H23WMxImyFZM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1628068602937,
     "user_tz": -330,
     "elapsed": 427,
     "user": {
      "displayName": "KANNUSAMY S KAPELESHH be15b012",
      "photoUrl": "",
      "userId": "02598409683794241244"
     }
    }
   },
   "source": [
    "file1 = open(filepath + \"stopwords/Japanese_stopword_list.txt\")\n",
    "line = file1.read()\n",
    "stopJa = line.split()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load English Stop words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "\n",
    "en_alphabets = ['o','q','w','e','r','t','y','u','i','p','a','s','d','f','g','h','j','k','l','z','x','c','v','b','n','m']\n",
    "sw_spacy.update(en_alphabets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import tokenizing functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rRNXiYP1yFZN"
   },
   "source": [
    "### Sudachi Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/.conda/envs/STS/lib/python3.7/site-packages/sudachipy/dictionarylib/doublearraylexicon.py\u001B[0m in \u001B[0;36mlookup\u001B[0;34m(self, text, offset)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mlength\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0moffset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mword_id\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mword_ids\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32myield\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mword_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlength\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_left_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword_id\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sudachipy.tokenizer.build_lattice_c'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kapeleshh/.conda/envs/STS/lib/python3.7/site-packages/sudachipy/dictionarylib/doublearraylexicon.py\", line 63, in lookup\n",
      "    yield (word_id, length)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "EOS is not connected to BOS",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-b78246fa7866>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdfs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msurface_sudachitokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/STS/tokenizers.py\u001B[0m in \u001B[0;36msurface_sudachitokens\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0ma\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0ma\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mneologdn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m         \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msurface\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mm\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokenizer_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'sudachi'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32msudachipy/tokenizer.pyx\u001B[0m in \u001B[0;36msudachipy.tokenizer.Tokenizer.tokenize\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32msudachipy/lattice.pyx\u001B[0m in \u001B[0;36msudachipy.lattice.Lattice.get_best_path\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: EOS is not connected to BOS"
     ]
    }
   ],
   "source": [
    "df_sudachi = surface_sudachitokens(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other tokenizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizers = [\"MeCab\", \"Janome\", \"nagisa\"]\n",
    "#tokenizers_support_postag = [\"MeCab\", \"Janome\", \"nagisa\"]\n",
    "\n",
    "word_tokenizers = []\n",
    "for word_tokenizer_name in tokenizers:\n",
    "    tokenizer = WordTokenizer(word_tokenizer_name)\n",
    "    word_tokenizers.append(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_tokens = multiple_tokenize(df, tokenizers, word_tokenizers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Concat all tokens in a dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_tokens, df_sudachi], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove Stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = df.columns[1:] # The 0th column is text and the rest are tokens\n",
    "df_all = stopwords_removal(df_all, cols, sw_spacy, stopJa, eng_words)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}